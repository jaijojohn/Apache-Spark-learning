{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaijojohn/Apache-Spark-learning/blob/main/End_To_End_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5u1fOqq7FkV",
        "outputId": "06a1bbc0-f8a0-48fa-ccb6-19c096cadc5b"
      },
      "id": "T5u1fOqq7FkV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [55.4 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [929 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,922 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,093 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,474 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,552 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,392 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,187 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Fetched 12.9 MB in 5s (2,604 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=ee29e093241b1464f5fe7d51f77839aea7b18e84e70f3d10663dc0ac961341b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1c59a1-7047-41f5-835a-0176d1e8b2e6",
      "metadata": {
        "id": "3c1c59a1-7047-41f5-835a-0176d1e8b2e6"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d9acb1-158a-4d2c-9bc1-9e457c58112d",
      "metadata": {
        "id": "f6d9acb1-158a-4d2c-9bc1-9e457c58112d"
      },
      "outputs": [],
      "source": [
        "flightData2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"data/flight-data/csv/2015-summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32881bb3-07d4-4a3a-be42-79fb0c873e0f",
      "metadata": {
        "id": "32881bb3-07d4-4a3a-be42-79fb0c873e0f",
        "outputId": "04038fcd-110b-4bcc-a29a-1983556333bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flightData2015.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2400c85f-c5df-400c-b171-3a8bc4c03849",
      "metadata": {
        "id": "2400c85f-c5df-400c-b171-3a8bc4c03849",
        "outputId": "b3f68708-cd86-488f-a83e-f7c7920e8a0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flightData2015.sort(\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b026a257-91f9-49fa-88e0-fa17c232a9f1",
      "metadata": {
        "id": "b026a257-91f9-49fa-88e0-fa17c232a9f1",
        "outputId": "ffc988d5-adea-43c8-c4e1-b35134cc5452"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=33]\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/darshil/Documents/DataWithDarshil/Apache Spark with DataBr..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015.sort(\"count\").explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7037d05f-98bd-48c5-a8cd-01a3513afc2a",
      "metadata": {
        "id": "7037d05f-98bd-48c5-a8cd-01a3513afc2a",
        "outputId": "60531769-7de9-4322-aca0-d121391d292a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|       United States|            Estonia|    1|\n",
            "|              Kosovo|      United States|    1|\n",
            "|              Zambia|      United States|    1|\n",
            "|       United States|   Papua New Guinea|    1|\n",
            "|               Malta|      United States|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "|            Suriname|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|            Djibouti|      United States|    1|\n",
            "|        Burkina Faso|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|             Cyprus|    1|\n",
            "|       United States|          Singapore|    1|\n",
            "|             Moldova|      United States|    1|\n",
            "|              Cyprus|      United States|    1|\n",
            "|       United States|          Lithuania|    1|\n",
            "|       United States|           Bulgaria|    1|\n",
            "|       United States|            Georgia|    1|\n",
            "|       United States|            Bahrain|    1|\n",
            "|       Cote d'Ivoire|      United States|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015.sort(\"count\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59f2d98-dab9-48a1-8c27-a3ea0101d2ef",
      "metadata": {
        "id": "f59f2d98-dab9-48a1-8c27-a3ea0101d2ef"
      },
      "outputs": [],
      "source": [
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b932ca-7e30-4e2d-804d-1b79343dcfdd",
      "metadata": {
        "id": "05b932ca-7e30-4e2d-804d-1b79343dcfdd"
      },
      "outputs": [],
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "    SELECT DEST_COUNTRY_NAME, count(1)\n",
        "    FROM flight_data_2015\n",
        "    GROUP BY DEST_COUNTRY_NAME\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435e58c9-a45b-478e-95f7-e055a7a63db2",
      "metadata": {
        "id": "435e58c9-a45b-478e-95f7-e055a7a63db2",
        "outputId": "85b07495-a341-48c8-b7ca-9fe660502127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=73]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/darshil/Documents/DataWithDarshil/Apache Spark with DataBr..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlWay.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad06731d-581f-4319-97b6-2498bc9525bf",
      "metadata": {
        "id": "ad06731d-581f-4319-97b6-2498bc9525bf"
      },
      "outputs": [],
      "source": [
        "dataFrameWay = flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e87c25e-63b6-478f-b138-907c687978b3",
      "metadata": {
        "id": "1e87c25e-63b6-478f-b138-907c687978b3",
        "outputId": "b225962d-9a0a-4f0a-9556-838d2be579a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=86]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/darshil/Documents/DataWithDarshil/Apache Spark with DataBr..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataFrameWay.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc7f565-2a3d-4188-b5e4-39e9c47c2ff0",
      "metadata": {
        "id": "5bc7f565-2a3d-4188-b5e4-39e9c47c2ff0",
        "outputId": "e6e8fc8f-2b14-4833-9fd4-66f329a0c1d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc19c214-7926-4b49-92e0-f97dea7dd0a5",
      "metadata": {
        "id": "bc19c214-7926-4b49-92e0-f97dea7dd0a5",
        "outputId": "67830b48-709a-4a28-f6e9-f839224b986b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.functions import max\n",
        "flightData2015.select(max(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94f82af-ec84-4b4f-a8a3-3bbaa1da42d8",
      "metadata": {
        "id": "e94f82af-ec84-4b4f-a8a3-3bbaa1da42d8",
        "outputId": "dd369d20-e9eb-47af-fd61-aaaa89e538a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "FROM flight_data_2015\n",
        "GROUP BY DEST_COUNTRY_NAME\n",
        "ORDER BY sum(count) DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "\n",
        "maxSql.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5966c6-3df6-4798-91ce-ef728130c30e",
      "metadata": {
        "id": "4b5966c6-3df6-4798-91ce-ef728130c30e",
        "outputId": "b5ef6378-b0cd-4d37-b8a5-c1cd90b6ec22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .sum(\"count\")\\\n",
        "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "    .sort(desc(\"destination_total\"))\\\n",
        "    .limit(5)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd525c3f-bd0f-4574-adab-374b13a341ba",
      "metadata": {
        "id": "bd525c3f-bd0f-4574-adab-374b13a341ba",
        "outputId": "26cc03ae-8525-4890-d5c8-7d93119fe5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#135L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#17,destination_total#135L])\n",
            "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[sum(count#19)])\n",
            "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=256]\n",
            "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_sum(count#19)])\n",
            "            +- FileScan csv [DEST_COUNTRY_NAME#17,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/darshil/Documents/DataWithDarshil/Apache Spark with DataBr..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .sum(\"count\")\\\n",
        "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "    .sort(desc(\"destination_total\"))\\\n",
        "    .limit(5)\\\n",
        "    .explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454fa12e-5967-4ec6-a04d-66c7ab546d93",
      "metadata": {
        "id": "454fa12e-5967-4ec6-a04d-66c7ab546d93"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}